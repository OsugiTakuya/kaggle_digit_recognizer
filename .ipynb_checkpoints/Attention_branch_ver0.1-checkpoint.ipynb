{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!ls\n",
    "・!でコマンド確認可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "Y_train = train[\"label\"]\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "# free some space\n",
    "del train \n",
    "g = sns.countplot(Y_train)\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・画像はグレースケール\n",
    "・trainには学習用の答えlabel\n",
    "・testにはlabelなし？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object\n",
      "**********\n",
      "count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the data\n",
    "print(X_train.isnull().any().describe())\n",
    "print(\"**********\")\n",
    "print(test.isnull().any().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノーマライゼーション：画素値を0-1に\n",
    "pythonのreshape(-1は？)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshapeで\n",
    "[データ数, height, width, color_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)\n",
    "\n",
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Split training and valdiation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 2\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainデータ：37800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some examples\n",
    "g = plt.imshow(X_train[1][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extractor Model\n",
    "# (input image)28pix * 28pix -> 7pix * 7pix (feature map)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1))) # 28 * 28\n",
    "model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu')) # 28 * 28* 32\n",
    "model.add(MaxPool2D(pool_size=(2,2))) # 28 * 28* 32, # MaxPool2DのストライドデフォルトはNoneで，pool_sizeと等しくなる．\n",
    "model.add(Dropout(0.25)) # 14 * 14 * 32 # Dropout (重みを指定した割合だけ0にして無効にする．)\n",
    "#この場合，1/4をランダムに無効にする．アンサンブルして結果を良くする．\n",
    "\n",
    "#計算②\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu')) # 14 * 14 * 32\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu')) # 14 * 14 * 64\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2))) # 14 * 14 * 64\n",
    "model.add(Dropout(0.25)) # 7 * 7 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perception Branch\n",
    "model.add(Flatten()) # 7 * 7 * 64\n",
    "model.add(Dense(256, activation = \"relu\")) # 出力は256個のノード # 3136 # Denseはaffineと同じように，ノードを結ぶ働きをする．\n",
    "model.add(Dropout(0.5)) # 256\n",
    "model.add(Dense(10, activation = \"softmax\"))# 出力は10個のノード # 256 # Denseは最後の層だけ使うのが普通．\n",
    "\n",
    "# Loss : L_per\n",
    "# Define the optimizer\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "#lr : 学習率\n",
    "# rho, epsilon : rmsiのパラメータ\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# softmaxは出力が10クラスの確率[0,0,0.8,0,0,0,0,0,0.2,0] = 足したら1\n",
    "# カテゴリカルクロスエントリピー ：ラベル[0,0,1,0,0,0,0,0,0,0] とエントロピー計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Branch\n",
    "\n",
    "# ★ここに入れるConvolution Layerの具体的なサイズは？\n",
    "\n",
    "# バッチノーマライゼーション追加\n",
    "model.add(Conv2D(filters = 1, kernel_size = (1,1),padding = 'Same', \n",
    "                 activation ='relu')) # 7 * 7 * 1\n",
    "\n",
    "# ★分岐\n",
    "model.add(Conv2D(filters = 1, kernel_size = (1,1),padding = 'Same', \n",
    "                 activation ='sigmoid')) # 7 * 7 * 1\n",
    "\n",
    "# バッチノーマライゼーション追加\n",
    "model.add(Conv2D(filters = 1, kernel_size = (1,1),padding = 'Same', \n",
    "                 activation ='sigmoid')) # 7 * 7 * 1\n",
    "\n",
    "# Loss : L_aten\n",
    "# Define the optimizer\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "#lr : 学習率\n",
    "# rho, epsilon : rmsiのパラメータ\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# softmaxは出力が10クラスの確率[0,0,0.8,0,0,0,0,0,0.2,0] = 足したら1\n",
    "# カテゴリカルクロスエントリピー ：ラベル[0,0,1,0,0,0,0,0,0,0] とエントロピー計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                        patience=3, verbose=1,factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "#          validation_data = (X_val, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With data augmentation to prevent overfitting (accuracy 0.99286)\n",
    "# datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         zoom_range = 0.1, # Randomly zoom image \n",
    "#         width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "#         height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "#         horizontal_flip=False,  # randomly flip images\n",
    "#         vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "# datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-54b73dab410d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                               , callbacks=[learning_rate_reduction])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelの構造保存\n",
    "json_string = model.to_json()\n",
    "open('digit_recognizer_model.json', 'w').write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重みの保存\n",
    "model.save_weights('digit_recognizer_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at confusion matrix \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes \"FROM\" one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations \"FROM\" one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "\n",
    "#あるカラムだけ1で他のカラムは0な行列の表現。カテゴリー変数でよく使います。\n",
    "#古典的な統計の教科書では「ダミー変数」という言い方もします。\n",
    "#PandasのOneHotベクトルを作る関数get_dummiesはこれが由来です。\n",
    "\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error見る．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelから行う方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 28, 28, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(test)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル構造ファイルと重みから予測する方法\n",
    "モデル構造：digit_recognizer_model.json <br>\n",
    "重みファイル：digit_recognizer_weights.h5 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの読み込み\n",
    "model = model_from_json(open('digit_recognizer_model.json', 'r').read())\n",
    "\n",
    "#重みの読み込み\n",
    "model.load_weights('mnist_mlp_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test[0:100].shape)\n",
    "results = model.predict(test[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "[2 0 9 0 3 7 0 3 0 3 5 7 4 0 4 3 3 1 9 0 9 1 1 5 7 4 2 7 4 7 7 5 4 2 6 2 5\n",
      " 5 1 6 7 7 4 9 8 7 8 2 6 7 6 8 8 3 8 2 1 2 2 0 4 1 7 0 0 0 1 9 0 1 6 5 8 8\n",
      " 2 8 9 9 2 3 5 4 1 0 9 2 4 3 6 7 2 0 6 6 1 4 3 9 7 4]\n"
     ]
    }
   ],
   "source": [
    "# data_num * class_num\n",
    "print(results.shape)\n",
    "\n",
    "predicted = np.argmax(results, axis = 1)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-Cam ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import keras\n",
    "import sys\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Cam_plus_plus(input_model, layer_name, x, row, col):\n",
    "\n",
    "    model = input_model\n",
    "\n",
    "    # 前処理\n",
    "    #X = np.expand_dims(x, axis=0)\n",
    "    #X = X.astype('float32')\n",
    "    #preprocessed_input = X / 255.0\n",
    "\n",
    "    # 予測クラスの算出\n",
    "    #predictions = model.predict(preprocessed_input)\n",
    "    print(\"入力：\" + str(x.shape))\n",
    "    predictions = model.predict(x)\n",
    "    class_idx = np.argmax(predictions[0])\n",
    "\n",
    "    #  使用する重みの抽出、高階微分の計算\n",
    "    class_output = model.layers[-1].output\n",
    "    conv_output = model.get_layer(layer_name).output\n",
    "    grads = K.gradients(class_output, conv_output)[0]\n",
    "    #first_derivative：１階微分\n",
    "    first_derivative = K.exp(class_output)[0][class_idx] * grads\n",
    "    #second_derivative：２階微分\n",
    "    second_derivative = K.exp(class_output)[0][class_idx] * grads * grads\n",
    "    #third_derivative：３階微分\n",
    "    third_derivative = K.exp(class_output)[0][class_idx] * grads * grads * grads\n",
    "\n",
    "    #関数の定義\n",
    "    gradient_function = K.function([model.input], [conv_output, first_derivative, second_derivative, third_derivative])  # model.inputを入力すると、conv_outputとgradsを出力する関数\n",
    "\n",
    "\n",
    "    conv_output, conv_first_grad, conv_second_grad, conv_third_grad = gradient_function([x])\n",
    "    conv_output, conv_first_grad, conv_second_grad, conv_third_grad = conv_output[0], conv_first_grad[0], conv_second_grad[0], conv_third_grad[0]\n",
    "\n",
    "    #alphaを求める\n",
    "    global_sum = np.sum(conv_output.reshape((-1, conv_first_grad.shape[2])), axis=0)\n",
    "    alpha_num = conv_second_grad\n",
    "    alpha_denom = conv_second_grad*2.0 + conv_third_grad*global_sum.reshape((1,1,conv_first_grad.shape[2]))\n",
    "    alpha_denom = np.where(alpha_denom!=0.0, alpha_denom, np.ones(alpha_denom.shape))\n",
    "    alphas = alpha_num / alpha_denom\n",
    "\n",
    "    #alphaの正規化\n",
    "    alpha_normalization_constant = np.sum(np.sum(alphas, axis = 0), axis = 0)\n",
    "    alpha_normalization_constant_processed = np.where(alpha_normalization_constant != 0.0, alpha_normalization_constant, np.ones(alpha_normalization_constant.shape))\n",
    "    alphas /= alpha_normalization_constant_processed.reshape((1,1,conv_first_grad.shape[2]))\n",
    "\n",
    "    #wの計算\n",
    "    weights = np.maximum(conv_first_grad, 0.0)\n",
    "    deep_linearization_weights = np.sum((weights * alphas).reshape((-1, conv_first_grad.shape[2])))\n",
    "\n",
    "    #Lの計算\n",
    "    grad_CAM_map = np.sum(deep_linearization_weights * conv_output, axis=2)\n",
    "    grad_CAM_map = np.maximum(grad_CAM_map, 0)\n",
    "    grad_CAM_map = grad_CAM_map / np.max(grad_CAM_map)\n",
    "    \n",
    "    #ヒートマップを描く\n",
    "    print(\"出力：\" + str(np.shape(grad_CAM_map)))\n",
    "    #grad_CAM_map = cv2.resize(grad_CAM_map, (row, col), cv2.INTER_LINEAR)\n",
    "    #jetcam = cv2.applyColorMap(np.uint8(255 * grad_CAM_map), cv2.COLORMAP_JET)  # モノクロ画像に疑似的に色をつける\n",
    "    #jetcam = (np.float32(jetcam) + x / 2)   # もとの画像に合成\n",
    "\n",
    "    return grad_CAM_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 887,530\n",
      "Trainable params: 887,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# layer_name : 最後のconvolution層直後のactivation層の名前を確認したい．\n",
    "# activation層（活性化関数を使っている層）がconvolution層に含まれている場合\n",
    "# ⇒ convolution層の名前でよい．\n",
    "# 層の名前はmodel.summary()で確認できる．\n",
    "model.summary()\n",
    "# これよりconv2d_4が対象のactivation層になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力：(5, 28, 28, 1)\n",
      "出力：(14, 14)\n"
     ]
    }
   ],
   "source": [
    "#次のTodo★\n",
    "#以下の関数を通るようにする．（上で実装したpredictionを通るようにする．）\n",
    " \n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from keras import backend as K\n",
    "\n",
    "# Model : 上で定義したmodel\n",
    "model = model\n",
    "\n",
    "#Model : VGG16 (.h5のダウンロードに10分かかる)\n",
    "#model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None)\n",
    "\n",
    "#Model : Resvet(.h5のダウンロードに10分かかる)\n",
    "#model = ResNet50(weights = 'imagenet')\n",
    "\n",
    "img = test[0:5] #shapeは datanum(5) * 28pix * 28pix * 1の形に\n",
    "\n",
    "target_layer = 'conv2d_4'\n",
    "row = 28\n",
    "col = 28\n",
    "\n",
    "img_GCAMplusplus = Grad_Cam_plus_plus(model, target_layer, img, row, col)\n",
    "#img_Gplusplusname = args.image_path+\"_GCAM++_%s.jpg\"%args.model\n",
    "#cv2.imwrite(img_Gplusplusname, img_GCAMplusplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次回Todo\n",
    "# cv2をimportして以下の計算を回す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot resize an array that references or is referenced\nby another array in this way.\nUse the np.resize function or refcheck=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f624fcb22e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_GCAMplusplus_resized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_GCAMplusplus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot resize an array that references or is referenced\nby another array in this way.\nUse the np.resize function or refcheck=False"
     ]
    }
   ],
   "source": [
    "img_GCAMplusplus_resized = img_GCAMplusplus.resize((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_CAM_map = cv2.resize(grad_CAM_map, (row, col), cv2.INTER_LINEAR)\n",
    "#jetcam = cv2.applyColorMap(np.uint8(255 * grad_CAM_map), cv2.COLORMAP_JET)  # モノクロ画像に疑似的に色をつける\n",
    "#jetcam = (np.float32(jetcam) + x / 2)   # もとの画像に合成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_GCAMplusplus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 : 線形補間<br>\n",
    "- https://qiita.com/ground0state/items/5fa0743837f1bcb374ca\n",
    "\n",
    "2 : テストデータとバリデーションデータの分割\n",
    "- from sklearn.model_selection import train_test_split\n",
    "\n",
    "3 : 混合行列の描写\n",
    " - 混同行列（confusion matrix）はクラス分類問題の結果を「実際のクラス」と「予測したクラス」を軸にしてまとめたもの。\n",
    " - from sklearn.metrics import confusion_matrix \n",
    " \n",
    "4 : ループ処理を楽にするitertools \n",
    " - https://qiita.com/__cooper/items/ff1d3d71088abb5d0849\n",
    "```\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "```\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "次回Todo : まとめ（from ３セル目寄り）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
