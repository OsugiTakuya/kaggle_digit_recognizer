{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!ls\n",
    "・!でコマンド確認可能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    4684\n",
       "7    4401\n",
       "3    4351\n",
       "9    4188\n",
       "2    4177\n",
       "6    4137\n",
       "0    4132\n",
       "4    4072\n",
       "8    4063\n",
       "5    3795\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the data\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "test = pd.read_csv(\"./test.csv\")\n",
    "\n",
    "Y_train = train[\"label\"]\n",
    "# Drop 'label' column\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "# free some space\n",
    "del train \n",
    "g = sns.countplot(Y_train)\n",
    "Y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "・画像はグレースケール\n",
    "・trainには学習用の答えlabel\n",
    "・testにはlabelなし？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object\n",
      "**********\n",
      "count       784\n",
      "unique        1\n",
      "top       False\n",
      "freq        784\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Check the data\n",
    "print(X_train.isnull().any().describe())\n",
    "print(\"**********\")\n",
    "print(test.isnull().any().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ノーマライゼーション：画素値を0-1に\n",
    "pythonのreshape(-1は？)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train = X_train / 255.0\n",
    "test = test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshapeで\n",
    "[データ数, height, width, color_channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "X_train = X_train.values.reshape(-1,28,28,1)\n",
    "test = test.values.reshape(-1,28,28,1)\n",
    "\n",
    "# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_train = to_categorical(Y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.6 Split training and valdiation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the random seed\n",
    "random_seed = 2\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "trainデータ：37800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Some examples\n",
    "g = plt.imshow(X_train[1][:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, merge\n",
    "import keras\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 次のTodo\n",
    "\n",
    "（実装）\n",
    "* kerasのプロパティとメソッドに目を通す（https://qiita.com/takurooo/items/f52c3cdad09da07f9d7f）\n",
    "* 済：kerasのグローバルアベレージをFeature_Extractorに実装してFeatureMapを見れるようにする．\n",
    "* 済：kerasのレイヤー分岐について調べて，簡単なレイヤー分岐を作る． (keras レイヤー分岐で検索)\n",
    "    - layer2 = Conv2D(***)(layer1) 等の引き継ぎに関して理解\n",
    "    - keras.layers.merge.add, keras.layers.merge.concatenate を使ってレイヤーの統合を理解する． おそらくmodel = merge([processed_a, processed_b], mode='sum') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAPでFlattenする前の最終層をFeatureMapとして取り出す．\n",
    "- 済：seqential()でモデル定義\n",
    "    -> 1: attentionのGAPについて再度計算式を抑える．以下を参考に<br>\n",
    "    https://github.com/salty-vanilla/cnn_saliency/blob/master/mnist/cam_vis.ipynb <br>\n",
    "   https://github.com/salty-vanilla/cnn_saliency/blob/master/imagenet/cam.ipynb <br> \n",
    "- 済：28 * 28 のdigitiデータで訓練，jsonと重みファイルをゲット．以下のメソッドでkeras.modelクラスの構造と重みが得られるはず<br>\n",
    "    -> from keras.models import model_from_json <br>\n",
    "    -> model = model_from_json(open('digit_recognizer_model.json', 'r').read()) <br>\n",
    "    -> model.load_weights('mnist_mlp_weights.h5') <br>\n",
    "- 済：指定した部分までの重みをかけた結果を取得．以下のメッソドようなものが使えるはず．<br>\n",
    "    -> class_output = model.layers[-1].output <br>\n",
    "    -> conv_output = model.get_layer(layer_name).output <br>\n",
    "- 済：分岐型ネットワークでGAPを実装したモデルを書き直す\n",
    "- 済：json, h5ファイルを再度得る．(まとめも兼ねて)\n",
    "- 済：中間層途中結果 & 中間層重み を取得し，\\sum_{channel}^{class} w_{channnel}^{class} * f_{channel}(x,y)を計算\n",
    "- 済：FeatureMapを描写\n",
    "\n",
    "- ★ Perception_Branchの実装\n",
    "\n",
    "(質問)\n",
    "- 重みwのshapeについて\n",
    "\n",
    "(調査)\n",
    "- kerasAPIを使った重み取得方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAM\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D,\\\n",
    "                        MaxPool2D, GlobalAveragePooling2D, BatchNormalization\n",
    "original_im = Input(shape=(28,28,1))\n",
    "x = Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same',\n",
    "           activation='relu', input_shape = (28,28,1))(original_im)\n",
    "x = Conv2D(filters = 64, kernel_size = (5,5,), padding = \"Same\",\n",
    "           activation=\"relu\")(x)\n",
    "x = MaxPool2D(pool_size = (2,2), strides = (2,2))(x)\n",
    "x = GlobalAveragePooling2D(data_format = None)(x) #Flattenはしなくてもいい．\n",
    "output = Dense(10, activation = \"softmax\")(x) # Denseは全結合層\n",
    "\n",
    "model = keras.models.Model(inputs = original_im, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 52,746\n",
      "Trainable params: 52,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=[\"categorical_crossentropy\"],metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                        patience=3, verbose=1,factor=0.5, min_lr=0.00001)\n",
    "\n",
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Y_trainはリスト2つに\n",
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelの構造保存\n",
    "json_string = model.to_json()\n",
    "open('digit_recognizer_model.json', 'w').write(json_string)\n",
    "\n",
    "#重みの保存\n",
    "model.save_weights('digit_recognizer_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルと重みの読込，\n",
    "from keras.models import model_from_json\n",
    "model = model_from_json(open('digit_recognizer_model.json', 'r').read())\n",
    "model.load_weights('digit_recognizer_weights.h5')\n",
    "\n",
    "#summaryをテキストとして保存\n",
    "with open(\"output.txt\", \"w\") as fp:\n",
    "    model.summary(print_fn=lambda x: fp.write(x + \"\\r\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 14, 14, 64)\n",
      "1/1 [==============================] - 0s 10ms/step\n",
      "(1, 1, 14, 14, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K #Keras API\n",
    "\n",
    "#中間層の結果を取得\n",
    "layer_name = 'max_pooling2d_8'\n",
    "intermediate_layer_model = Model(inputs=model.input,\n",
    "                                 outputs=model.get_layer(layer_name).output)\n",
    "intermediate_output = intermediate_layer_model.predict(X_train[0:1])\n",
    "print(np.shape(intermediate_output))\n",
    "\n",
    "# Keras APIを使用\n",
    "ret = model.predict(X_train[0:1], 1, 1)\n",
    "get_layer_output = K.function([model.get_layer(layer_name).input],\n",
    "                                  [model.get_layer(layer_name).output])\n",
    "layer_output = get_layer_output([X_train[0:1],])\n",
    "print(np.shape(layer_output))\n",
    "np.save('./max_pooling2d_10.npy', layer_output[0], allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 1, 32)\n",
      "(32,)\n",
      "(5, 5, 32, 64)\n",
      "(64,)\n",
      "(64, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "#中間の重みを取得\n",
    "w = model.get_weights()\n",
    "for i in range(len(w)):\n",
    "    print(w[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 10)\n",
      "(14, 14, 64)\n",
      "(14, 14, 10)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# FeatureMapを10クラス分計算\n",
    "print(w[-2].shape)\n",
    "print(intermediate_output[0].shape)\n",
    "\n",
    "# 全クラスのfeaturemap計算\n",
    "feature_map, = intermediate_output @ w[-2]\n",
    "print(feature_map.shape)\n",
    "for i in range(10):\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(feature_map[:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attention Branch Network\n",
    "\n",
    "# 分岐型ネットワークでの定義\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D,\\\n",
    "                        MaxPool2D, GlobalAveragePooling2D, BatchNormalization\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "original_im = Input(shape=(28,28,1))\n",
    "x = Conv2D(filters = 32, kernel_size = (5,5), padding = 'Same',\n",
    "           activation='relu', input_shape = (28,28,1))(original_im)\n",
    "x = Conv2D(filters = 64, kernel_size = (5,5,), padding = \"Same\",\n",
    "           activation=\"relu\")(x)\n",
    "x1 = MaxPool2D(pool_size = (2,2), strides = (2,2))(x) #14*14*64\n",
    "\n",
    "# Attention Branch\n",
    "x2 = Conv2D(filters = 64, kernel_size = (5,5,), padding = \"Same\",\n",
    "           activation=\"relu\")(x1)\n",
    "x2 = Conv2D(filters = 64, kernel_size = (1,1,), padding = \"Same\",\n",
    "           activation=\"relu\")(x2)\n",
    "\n",
    "# Attention Branch 1\n",
    "x2_1 = Conv2D(filters = 64, kernel_size = (1,1,), padding = \"Same\",\n",
    "           activation=\"sigmoid\")(x2) #14*14*64\n",
    "\n",
    "# Attention Branch 2\n",
    "x2_2 = Conv2D(filters = 64, kernel_size = (1,1,), padding = \"Same\",\n",
    "           activation=\"relu\")(x2)\n",
    "x2_2 = GlobalAveragePooling2D(data_format = None)(x2_2) \n",
    "output1 = Dense(10, activation = \"softmax\")(x2_2)\n",
    "\n",
    "# Perception Branch\n",
    "#ここにx1とx2_1を足し合わせる記述を記載\n",
    "x4 = keras.layers.multiply([x1, x2_1]) #アダマール積\n",
    "#x4 = keras.layers.add([x1, x4 + 1])\n",
    "x4 = Dense(512, activation = \"relu\")(x4) \n",
    "output2 = Dense(10, activation = \"softmax\")(x4)\n",
    "\n",
    "model = keras.models.Model(inputs = original_im, outputs = [output1, output2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_18 (InputLayer)           (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 28, 28, 32)   832         input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 28, 28, 64)   51264       conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling2D) (None, 14, 14, 64)   0           conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 14, 14, 64)   102464      max_pooling2d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 14, 14, 64)   4160        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 14, 14, 64)   4160        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 14, 14, 64)   4160        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_7 (Multiply)           (None, 14, 14, 64)   0           max_pooling2d_14[0][0]           \n",
      "                                                                 conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_14 (Gl (None, 64)           0           conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 14, 14, 512)  33280       multiply_7[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 10)           650         global_average_pooling2d_14[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 14, 14, 10)   5130        dense_27[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 206,100\n",
      "Trainable params: 206,100\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0....",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-58c403d3f594>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                               , callbacks=[learning_rate_reduction]) \n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    148\u001b[0m                                      str(validation_data))\n\u001b[1;32m    149\u001b[0m                 val_x, val_y, val_sample_weights = model._standardize_user_data(\n\u001b[0;32m--> 150\u001b[0;31m                     val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0mval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 if model.uses_learning_phase and not isinstance(K.learning_phase(),\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model target: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 1 arrays: [array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 1.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 1., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0...."
     ]
    }
   ],
   "source": [
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=[\"categorical_crossentropy\", \"categorical_crossentropy\"],metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                        patience=3, verbose=1,factor=0.5, min_lr=0.00001)\n",
    "\n",
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=False,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "datagen.fit(X_train)\n",
    "\n",
    "# Y_trainはリスト2つに\n",
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val, Y_val),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 28, 28, 32)   832         input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 28, 28, 64)   51264       conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 14, 14, 64)   0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 14, 14, 64)   102464      max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 14, 14, 64)   4160        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 14, 14, 64)   4160        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 14, 14, 64)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_add_2 (TensorFlowOp [(None, 14, 14, 64)] 0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 14, 14, 64)   0           max_pooling2d_2[0][0]            \n",
      "                                                                 tf_op_layer_add_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 14, 14, 512)  33280       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 14, 14, 10)   5130        dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 201,290\n",
      "Trainable params: 201,290\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 28, 28, 64)        51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_8 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 52,746\n",
      "Trainable params: 52,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "# (input image)28pix * 28pix -> 7pix * 7pix (feature map)\n",
    "Feature_Extractor = Sequential()\n",
    "Feature_Extractor.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1))) # 28 * 28\n",
    "Feature_Extractor.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu')) # 28 * 28* 32\n",
    "Feature_Extractor.add(MaxPool2D(pool_size=(2,2))) # 28 * 28* 32, # MaxPool2DのストライドデフォルトはNoneで，pool_sizeと等しくなる．\n",
    "Feature_Extractor.add(Dropout(0.25)) # 14 * 14 * 32 # Dropout (重みを指定した割合だけ0にして無効にする．)\n",
    "#この場合，1/4をランダムに無効にする．アンサンブルして結果を良くする．\n",
    "\n",
    "#計算②\n",
    "Feature_Extractor.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu')) # 14 * 14 * 32\n",
    "Feature_Extractor.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu')) # 14 * 14 * 64\n",
    "Feature_Extractor.add(MaxPool2D(pool_size=(2,2), strides=(2,2))) # 14 * 14 * 64\n",
    "Feature_Extractor.add(Dropout(0.25)) # 7 * 7 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Extractor Layer　（完成）\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "# (input image)28pix * 28pix -> 7pix * 7pix (feature map)\n",
    "Feature_Extractor = Sequential()\n",
    "Feature_Extractor.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu', input_shape = (28,28,1))) # 28 * 28\n",
    "Feature_Extractor.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n",
    "                 activation ='relu')) # 28 * 28* 32\n",
    "Feature_Extractor.add(MaxPool2D(pool_size=(2,2))) # 28 * 28* 32, # MaxPool2DのストライドデフォルトはNoneで，pool_sizeと等しくなる．\n",
    "Feature_Extractor.add(Dropout(0.25)) # 14 * 14 * 32 # Dropout (重みを指定した割合だけ0にして無効にする．)\n",
    "#この場合，1/4をランダムに無効にする．アンサンブルして結果を良くする．\n",
    "\n",
    "#計算②\n",
    "Feature_Extractor.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu')) # 14 * 14 * 32\n",
    "Feature_Extractor.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
    "                 activation ='relu')) # 14 * 14 * 64\n",
    "Feature_Extractor.add(MaxPool2D(pool_size=(2,2), strides=(2,2))) # 14 * 14 * 64\n",
    "Feature_Extractor.add(Dropout(0.25)) # 7 * 7 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer conv2d_13 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.engine.sequential.Sequential'>. Full input: [<keras.engine.sequential.Sequential object at 0x7f2022a57b38>]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m                 \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_keras_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mis_keras_tensor\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    696\u001b[0m         raise ValueError('Unexpectedly found an instance of type `' +\n\u001b[0;32m--> 697\u001b[0;31m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m                          'Expected a symbolic tensor instance.')\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpectedly found an instance of type `<class 'keras.engine.sequential.Sequential'>`. Expected a symbolic tensor instance.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ca4bd7c28dac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Perception Branch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mPer_Branch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeature_Extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mPer_Branch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeature_Extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 7 * 7 * 64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPer_Branch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 出力は256個のノード # 3136 # Denseはaffineと同じように，ノードを結ぶ働きをする．\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mPer_Branch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer conv2d_13 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.engine.sequential.Sequential'>. Full input: [<keras.engine.sequential.Sequential object at 0x7f2022a57b38>]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "# Perception Branch\n",
    "Per_Branch = Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu')(Feature_Extractor)\n",
    "Per_Branch.add(Flatten(Feature_Extractor)) # 7 * 7 * 64\n",
    "Per_Branch.add(Dense(256, activation = \"relu\")) # 出力は256個のノード # 3136 # Denseはaffineと同じように，ノードを結ぶ働きをする．\n",
    "Per_Branch.add(Dropout(0.5)) # 256\n",
    "Per_Branch.add(Dense(10, activation = \"softmax\"))# 出力は10個のノード # 256 # Denseは最後の層だけ使うのが普通．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention Branch\n",
    "\n",
    "# ★ここに入れるConvolution Layerの具体的なサイズは？ \n",
    "# -> Feature MapとAttention Mapが同じになるなら適当で良し．\n",
    "\n",
    "# バッチノーマライゼーション追加\n",
    "model.add(Conv2D(filters = 1, kernel_size = (1,1),padding = 'Same', \n",
    "                 activation ='relu')) # 7 * 7 * 1\n",
    "\n",
    "# ★分岐\n",
    "model.add(Conv2D(filters = 1, kernel_size = (1,1),padding = 'Same', \n",
    "                 activation ='sigmoid')) # 7 * 7 * 1\n",
    "\n",
    "# バッチノーマライゼーション追加\n",
    "model.add(Conv2D(filters = 1, kernel_size = (1,1),padding = 'Same', \n",
    "                 activation ='sigmoid')) # 7 * 7 * 1\n",
    "\n",
    "# Loss : L_aten\n",
    "# Define the optimizer\n",
    "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "#lr : 学習率\n",
    "# rho, epsilon : rmsiのパラメータ\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "# softmaxは出力が10クラスの確率[0,0,0.8,0,0,0,0,0,0.2,0] = 足したら1\n",
    "# カテゴリカルクロスエントリピー ：ラベル[0,0,1,0,0,0,0,0,0,0] とエントロピー計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Model(inputs = \"hoge\", outputs = [\"hogehoge\", \"hogehoge\"])\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=['categorical_crossentropy', 'categorical_crossentropy'],\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# Set a learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n",
    "                        patience=3, verbose=1,factor=0.5, min_lr=0.00001)\n",
    "\n",
    "epochs = 1 # Turn epochs to 30 to get 0.9967 accuracy\n",
    "batch_size = 86\n",
    "\n",
    "# Y_trainはリスト2つに\n",
    "history = model.fit_generator(datagen.flow(X_train,[Y_train,Y_train], batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n",
    "#          validation_data = (X_val, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With data augmentation to prevent overfitting (accuracy 0.99286)\n",
    "# datagen = ImageDataGenerator(\n",
    "#         featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "#         samplewise_center=False,  # set each sample mean to 0\n",
    "#         featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "#         samplewise_std_normalization=False,  # divide each input by its std\n",
    "#         zca_whitening=False,  # apply ZCA whitening\n",
    "#         rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "#         zoom_range = 0.1, # Randomly zoom image \n",
    "#         width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "#         height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "#         horizontal_flip=False,  # randomly flip images\n",
    "#         vertical_flip=False)  # randomly flip images\n",
    "\n",
    "\n",
    "# datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここで計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-54b73dab410d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                               , callbacks=[learning_rate_reduction])\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "history = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n",
    "                              epochs = epochs, validation_data = (X_val,Y_val),\n",
    "                              verbose = 2, steps_per_epoch=X_train.shape[0] // batch_size\n",
    "                              , callbacks=[learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelの構造保存\n",
    "json_string = model.to_json()\n",
    "open('digit_recognizer_model.json', 'w').write(json_string)\n",
    "\n",
    "#重みの保存\n",
    "model.save_weights('digit_recognizer_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the loss and accuracy curves for training and validation \n",
    "fig, ax = plt.subplots(2,1)\n",
    "ax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
    "legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "ax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "legend = ax[1].legend(loc='best', shadow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look at confusion matrix \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "Y_pred = model.predict(X_val)\n",
    "# Convert predictions classes \"FROM\" one hot vectors \n",
    "Y_pred_classes = np.argmax(Y_pred,axis = 1) \n",
    "# Convert validation observations \"FROM\" one hot vectors\n",
    "Y_true = np.argmax(Y_val,axis = 1) \n",
    "\n",
    "#あるカラムだけ1で他のカラムは0な行列の表現。カテゴリー変数でよく使います。\n",
    "#古典的な統計の教科書では「ダミー変数」という言い方もします。\n",
    "#PandasのOneHotベクトルを作る関数get_dummiesはこれが由来です。\n",
    "\n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "error見る．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (Y_pred_classes - Y_true != 0)\n",
    "\n",
    "Y_pred_classes_errors = Y_pred_classes[errors]\n",
    "Y_pred_errors = Y_pred[errors]\n",
    "Y_true_errors = Y_true[errors]\n",
    "X_val_errors = X_val[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modelから行う方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 28, 28, 1)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict results\n",
    "results = model.predict(test)\n",
    "\n",
    "# select the indix with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "\n",
    "submission.to_csv(\"cnn_mnist_datagen.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデル構造ファイルと重みから予測する方法\n",
    "モデル構造：digit_recognizer_model.json <br>\n",
    "重みファイル：digit_recognizer_weights.h5 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "# モデルの読み込み\n",
    "model = model_from_json(open('digit_recognizer_model.json', 'r').read())\n",
    "\n",
    "#重みの読み込み\n",
    "model.load_weights('mnist_mlp_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(test[0:100].shape)\n",
    "results = model.predict(test[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "[2 0 9 0 3 7 0 3 0 3 5 7 4 0 4 3 3 1 9 0 9 1 1 5 7 4 2 7 4 7 7 5 4 2 6 2 5\n",
      " 5 1 6 7 7 4 9 8 7 8 2 6 7 6 8 8 3 8 2 1 2 2 0 4 1 7 0 0 0 1 9 0 1 6 5 8 8\n",
      " 2 8 9 9 2 3 5 4 1 0 9 2 4 3 6 7 2 0 6 6 1 4 3 9 7 4]\n"
     ]
    }
   ],
   "source": [
    "# data_num * class_num\n",
    "print(results.shape)\n",
    "\n",
    "predicted = np.argmax(results, axis = 1)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grad-Cam ++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import keras\n",
    "import sys\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grad_Cam_plus_plus(input_model, layer_name, x, row, col):\n",
    "\n",
    "    model = input_model\n",
    "\n",
    "    # 前処理\n",
    "    #X = np.expand_dims(x, axis=0)\n",
    "    #X = X.astype('float32')\n",
    "    #preprocessed_input = X / 255.0\n",
    "\n",
    "    # 予測クラスの算出\n",
    "    #predictions = model.predict(preprocessed_input)\n",
    "    print(\"入力：\" + str(x.shape))\n",
    "    predictions = model.predict(x)\n",
    "    class_idx = np.argmax(predictions[0])\n",
    "\n",
    "    #  使用する重みの抽出、高階微分の計算\n",
    "    class_output = model.layers[-1].output\n",
    "    conv_output = model.get_layer(layer_name).output\n",
    "    grads = K.gradients(class_output, conv_output)[0]\n",
    "    #first_derivative：１階微分\n",
    "    first_derivative = K.exp(class_output)[0][class_idx] * grads\n",
    "    #second_derivative：２階微分\n",
    "    second_derivative = K.exp(class_output)[0][class_idx] * grads * grads\n",
    "    #third_derivative：３階微分\n",
    "    third_derivative = K.exp(class_output)[0][class_idx] * grads * grads * grads\n",
    "\n",
    "    #関数の定義\n",
    "    gradient_function = K.function([model.input], [conv_output, first_derivative, second_derivative, third_derivative])  # model.inputを入力すると、conv_outputとgradsを出力する関数\n",
    "\n",
    "\n",
    "    conv_output, conv_first_grad, conv_second_grad, conv_third_grad = gradient_function([x])\n",
    "    conv_output, conv_first_grad, conv_second_grad, conv_third_grad = conv_output[0], conv_first_grad[0], conv_second_grad[0], conv_third_grad[0]\n",
    "\n",
    "    #alphaを求める\n",
    "    global_sum = np.sum(conv_output.reshape((-1, conv_first_grad.shape[2])), axis=0)\n",
    "    alpha_num = conv_second_grad\n",
    "    alpha_denom = conv_second_grad*2.0 + conv_third_grad*global_sum.reshape((1,1,conv_first_grad.shape[2]))\n",
    "    alpha_denom = np.where(alpha_denom!=0.0, alpha_denom, np.ones(alpha_denom.shape))\n",
    "    alphas = alpha_num / alpha_denom\n",
    "\n",
    "    #alphaの正規化\n",
    "    alpha_normalization_constant = np.sum(np.sum(alphas, axis = 0), axis = 0)\n",
    "    alpha_normalization_constant_processed = np.where(alpha_normalization_constant != 0.0, alpha_normalization_constant, np.ones(alpha_normalization_constant.shape))\n",
    "    alphas /= alpha_normalization_constant_processed.reshape((1,1,conv_first_grad.shape[2]))\n",
    "\n",
    "    #wの計算\n",
    "    weights = np.maximum(conv_first_grad, 0.0)\n",
    "    deep_linearization_weights = np.sum((weights * alphas).reshape((-1, conv_first_grad.shape[2])))\n",
    "\n",
    "    #Lの計算\n",
    "    grad_CAM_map = np.sum(deep_linearization_weights * conv_output, axis=2)\n",
    "    grad_CAM_map = np.maximum(grad_CAM_map, 0)\n",
    "    grad_CAM_map = grad_CAM_map / np.max(grad_CAM_map)\n",
    "    \n",
    "    #ヒートマップを描く\n",
    "    print(\"出力：\" + str(np.shape(grad_CAM_map)))\n",
    "    #grad_CAM_map = cv2.resize(grad_CAM_map, (row, col), cv2.INTER_LINEAR)\n",
    "    #jetcam = cv2.applyColorMap(np.uint8(255 * grad_CAM_map), cv2.COLORMAP_JET)  # モノクロ画像に疑似的に色をつける\n",
    "    #jetcam = (np.float32(jetcam) + x / 2)   # もとの画像に合成\n",
    "\n",
    "    return grad_CAM_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 32)        832       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               803072    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 887,530\n",
      "Trainable params: 887,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# layer_name : 最後のconvolution層直後のactivation層の名前を確認したい．\n",
    "# activation層（活性化関数を使っている層）がconvolution層に含まれている場合\n",
    "# ⇒ convolution層の名前でよい．\n",
    "# 層の名前はmodel.summary()で確認できる．\n",
    "model.summary()\n",
    "# これよりconv2d_4が対象のactivation層になる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "入力：(5, 28, 28, 1)\n",
      "出力：(14, 14)\n"
     ]
    }
   ],
   "source": [
    "#次のTodo★\n",
    "#以下の関数を通るようにする．（上で実装したpredictionを通るようにする．）\n",
    " \n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from keras import backend as K\n",
    "\n",
    "# Model : 上で定義したmodel\n",
    "model = model\n",
    "\n",
    "#Model : VGG16 (.h5のダウンロードに10分かかる)\n",
    "#model = VGG16(include_top=True, weights='imagenet', input_tensor=None, input_shape=None)\n",
    "\n",
    "#Model : Resvet(.h5のダウンロードに10分かかる)\n",
    "#model = ResNet50(weights = 'imagenet')\n",
    "\n",
    "img = test[0:5] #shapeは datanum(5) * 28pix * 28pix * 1の形に\n",
    "\n",
    "target_layer = 'conv2d_4'\n",
    "row = 28\n",
    "col = 28\n",
    "\n",
    "img_GCAMplusplus = Grad_Cam_plus_plus(model, target_layer, img, row, col)\n",
    "#img_Gplusplusname = args.image_path+\"_GCAM++_%s.jpg\"%args.model\n",
    "#cv2.imwrite(img_Gplusplusname, img_GCAMplusplus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 次回Todo\n",
    "# cv2をimportして以下の計算を回す．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot resize an array that references or is referenced\nby another array in this way.\nUse the np.resize function or refcheck=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-f624fcb22e69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimg_GCAMplusplus_resized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_GCAMplusplus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot resize an array that references or is referenced\nby another array in this way.\nUse the np.resize function or refcheck=False"
     ]
    }
   ],
   "source": [
    "img_GCAMplusplus_resized = img_GCAMplusplus.resize((28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_CAM_map = cv2.resize(grad_CAM_map, (row, col), cv2.INTER_LINEAR)\n",
    "#jetcam = cv2.applyColorMap(np.uint8(255 * grad_CAM_map), cv2.COLORMAP_JET)  # モノクロ画像に疑似的に色をつける\n",
    "#jetcam = (np.float32(jetcam) + x / 2)   # もとの画像に合成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 14)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_GCAMplusplus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIPS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 : 線形補間<br>\n",
    "- https://qiita.com/ground0state/items/5fa0743837f1bcb374ca\n",
    "\n",
    "2 : テストデータとバリデーションデータの分割\n",
    "- from sklearn.model_selection import train_test_split\n",
    "\n",
    "3 : 混合行列の描写\n",
    " - 混同行列（confusion matrix）はクラス分類問題の結果を「実際のクラス」と「予測したクラス」を軸にしてまとめたもの。\n",
    " - from sklearn.metrics import confusion_matrix \n",
    " \n",
    "4 : ループ処理を楽にするitertools \n",
    " - https://qiita.com/__cooper/items/ff1d3d71088abb5d0849\n",
    "```\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "```\n",
    "\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "X_train = train.drop(labels = [\"label\"],axis = 1) \n",
    "\n",
    "次回Todo : まとめ（from ３セル目寄り）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
